# -*- coding: utf-8 -*-
"""Applied_ML_project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iOIQSag71nQNp4WaRCbMFQnT-PAxWHfw

## Importing libraries
"""

import numpy as np
from collections import Counter
import pandas as pd
from math import sqrt
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/gdrive')

"""## Getting the dats"""

full_datas = np.loadtxt('/content/gdrive/MyDrive/ml-25m/ml-latest-small/ratings.csv', delimiter = ',', skiprows=1, dtype=int)

full_datas

datas = full_datas[:, :3]

#datas = inter[inter[:,0].argsort()]

datas1M = np.genfromtxt('/content/gdrive/MyDrive/ml-1m/ratings.dat', delimiter='::').astype(np.int64)[:,:3]

"""## Mapping function for obtaining datas sparse both by user and by movies"""

def get_sparse_matrix(data):
    # Initialize dictionaries to map users and movies to indices
    user_to_index = {}
    movie_to_index = {}
    list_user_index = []
    list_movie_index = []

    matrix_by_user = []
    matrix_by_movie = []

    for user, movie, rating in data:
        # Map user and movie to indices
        if user not in user_to_index:
            user_to_index[user] = len(list_user_index)
            list_user_index.append(user)
            matrix_by_user.append([])
        user_index = user_to_index[user]

        if movie not in movie_to_index:
            movie_to_index[movie] = len(list_movie_index)
            list_movie_index.append(movie)
            matrix_by_movie.append([])
        movie_index = movie_to_index[movie]

        # Append the rating to the corresponding matrices
        matrix_by_user[user_index].append((movie_index, rating))
        matrix_by_movie[movie_index].append((user_index, rating))

    return matrix_by_user, matrix_by_movie, list_user_index, list_movie_index

matrix_by_user, matrix_by_movie, list_user_index, list_movie_index = get_sparse_matrix(datas)

"""## Spliting the dats"""

def create_sparse_matrix(rating_data):
  # Shuffling and splitting point
  np.random.shuffle(rating_data)

  split_point = 0.8 * len(rating_data)

  # Initialization
  user_sys_to_id = {}
  user_id_to_sys = []
  movie_sys_to_id = {}
  movie_id_to_sys = []

  # First build the mappings.
  for index in range(len(rating_data)):

    user_sys = rating_data[index][0]
    movie_sys = rating_data[index][1]

    #Take care of the user data structure
    if user_sys not in user_sys_to_id:
      user_id_to_sys.append(user_sys)
      user_sys_to_id[user_sys] = len(user_sys_to_id)

    #Take care of the movie data structure
    if movie_sys not in movie_sys_to_id:
      movie_id_to_sys.append(movie_sys)
      movie_sys_to_id[movie_sys]=len(movie_sys_to_id)

  # Initialize with empty list all the trainings data.
  data_by_user_train = [[] for i in range(len(user_id_to_sys))]
  data_by_movie_train = [[] for i in range(len(movie_id_to_sys))]

  # Initialize with empty list all the test data.
  data_by_user_test = [[] for i in range(len(user_id_to_sys))]
  data_by_movie_test = [[] for i in range(len(movie_id_to_sys))]

  # Create all the data structure using in a loop
  for index in range(len(rating_data)):

    user_sys = rating_data[index][0]
    movie_sys = rating_data[index][1]
    rating = rating_data[index][2]

    user_index = user_sys_to_id[user_sys]
    movie_index = movie_sys_to_id[movie_sys]

    if index < split_point:
      # Insert into the sparse user and item *training* matrices.
      data_by_user_train[user_index].append((movie_index, float(rating)))
      data_by_movie_train[movie_index].append((user_index, float(rating)))

    else:
      # Insert into the sparse user and item *test* matrices.
      data_by_user_test[user_index].append((movie_index, float(rating)))
      data_by_movie_test[movie_index].append((user_index,float(rating)))

  return user_sys_to_id, user_id_to_sys, movie_sys_to_id, movie_id_to_sys, data_by_user_train, data_by_movie_train, data_by_user_test, data_by_movie_test

import random

def split_sparse_matrices(matrix_by_user, matrix_by_movie, split_ratio=0.85):
    user_train_matrix = []
    user_test_matrix = []
    movie_train_matrix = []
    movie_test_matrix = []

    for user_data in matrix_by_user:
        # Shuffle the user data
        random.shuffle(user_data)
        split_point = int(len(user_data) * split_ratio)
        train_data = user_data[:split_point]
        test_data = user_data[split_point:]
        user_train_matrix.append(train_data)
        user_test_matrix.append(test_data)

    for movie_data in matrix_by_movie:
        # Shuffle the movie data
        random.shuffle(movie_data)
        split_point = int(len(movie_data) * split_ratio)
        train_data = movie_data[:split_point]
        test_data = movie_data[split_point:]
        movie_train_matrix.append(train_data)
        movie_test_matrix.append(test_data)

    return user_train_matrix, user_test_matrix, movie_train_matrix, movie_test_matrix

data_by_user_train, data_by_user_test, data_by_movie_train, data_by_movie_test = split_sparse_matrices(matrix_by_user, matrix_by_movie, split_ratio=0.85)

"""## Loading the (25M) movielens datas"""

datas_25M = (pd.read_csv('/content/gdrive/MyDrive/ml-25m/ratings.csv', usecols = ['userId', 'movieId', 'rating'])).to_numpy() # first loading as a data frame

datas_25M.shape

matrix_by_user, matrix_by_movie = get_sparse_matrices(datas_25M)

matrix_by_user

"""## **Plotings the power law**"""

def power_law_plot(matrix1, matrix2):

    import seaborn as sns

    num_of_rat_user = [len(i) for i in matrix1]
    count_occ_user = np.bincount(num_of_rat_user)

    num_rate_movie = [len(i) for i in matrix2]
    count_occ_movie = np.bincount(num_rate_movie)


    sns.scatterplot(x = np.log(range(len(count_occ_user))), y = np.log(count_occ_user))
    sns.scatterplot(x = np.log(range(len(count_occ_movie))), y = np.log(count_occ_movie))

    plt.legend(labels=['User Ratings', 'Movie Ratings'])


    plt.xlabel('Number of ratings')
    plt.ylabel('Frequency')
    plt.title('Power law distribution')
    plt.show()

power_law_plot(matrix_by_user, matrix_by_movie)

"""## Buildind the model with the alterating least square algorithm

### Hyperparameters

### Initialization of vectors
"""

Lambda = 0.01
Gamma = 0.5
Tau = 0.01
number_of_iterations = 50
k = 5 # Number of latent factors


M = len(matrix_by_user) # number of users
N = len(matrix_by_movie) # number of movies

# Initialization of user and movie vectors
User_matrix = np.random.normal(0,1/np.sqrt(k),size=(M,k)) # initialization of users
Movie_matrix = np.random.normal(0,1/np.sqrt(k),size=(N,k))  # initialization of movies

# Initialization of bias
bias_u = np.zeros((M))  # initialization of users's bias
bias_m = np.zeros((N))  # initialization of movies' bias

bias_m.shape

"""### Necessary functions

### 1- Update user and movie matrices
"""

# Function for updating users

def update_user_matrices(matrix_by_user, user_matrix, movie_matrix, bias_u, bias_m):
    k = user_matrix.shape[1]

    for user_id in range(len(user_matrix)):
        if not matrix_by_user[user_id]:
            continue

        right_sum = 0
        left_sum = 0

        for movie_id, rating in matrix_by_user[user_id]:
            left_sum += Lambda * np.outer(movie_matrix[movie_id], movie_matrix[movie_id])
            right_sum += Lambda * movie_matrix[movie_id] * (rating - bias_u[user_id] - bias_m[movie_id])

        left_sum += Tau * np.eye(k)
        left_term = np.linalg.inv(left_sum)
        user_matrix[user_id] = np.matmul(left_term, right_sum)

    return user_matrix


# Function for updating movies

def update_movie_matrices(matrix_by_movie, user_matrix, movie_matrix, bias_u, bias_m):
    k = movie_matrix.shape[1]

    for movie_id in range(len(movie_matrix)):
        if not matrix_by_movie[movie_id]:
            continue

        right_sum = 0
        left_sum = 0

        for user_id, rating in matrix_by_movie[movie_id]:
            left_sum += Lambda * np.outer(user_matrix[user_id], user_matrix[user_id])
            right_sum += Lambda * user_matrix[user_id] * (rating - bias_m[movie_id] - bias_u[user_id])

        left_sum += Tau * np.eye(k)
        left_term = np.linalg.inv(left_sum)
        movie_matrix[movie_id] = np.matmul(left_term, right_sum)

    return movie_matrix

len(update_movie_matrices(matrix_by_movie, User_matrix, Movie_matrix, bias_u, bias_m))

"""### 3-Update bias"""

# Function for updating user biases

def update_user_biases(matrix_by_user, user_matrix, movie_matrix, bias_u, bias_m):
    for user_id in range(len(user_matrix)):
        bias = 0
        movie_counter = 0
        for movie_id, rating in matrix_by_user[user_id]:
            bias += Lambda * (rating - (np.matmul(user_matrix[user_id].T, movie_matrix[movie_id]) + bias_m[movie_id]))
            movie_counter += 1
        bias = bias / (Lambda * movie_counter + Gamma)
        bias_u[user_id] = bias

    return bias_u


# Function for updating movies bias

def update_movie_biases(matrix_by_movie, user_matrix, movie_matrix, bias_u, bias_m):
    for movie_id in range(len(movie_matrix)):
        bias = 0
        user_counter = 0
        for user_id, rating in matrix_by_movie[movie_id]:
            bias += Lambda * (rating - (np.matmul(movie_matrix[movie_id].T, user_matrix[user_id]) + bias_u[user_id]))
            user_counter += 1
        bias = bias / (Lambda * user_counter + Gamma)
        bias_m[movie_id] = bias

    return bias_m

"""### compute the loss"""

def compute_loss(matrix_by_user, matrix_by_movie, user_matrix, movie_matrix, bias_u, bias_m):

    # Initialisation
    sum = 0.0
    # Loss
    for user_id in range(len(user_matrix)):
        for movie_id, rating in matrix_by_user[user_id]:
            predicted_rating = np.dot(user_matrix[user_id].T, movie_matrix[movie_id]) + bias_u[user_id] + bias_m[movie_id]
            prediction_error = rating - predicted_rating
            sum += prediction_error**2

    #  Regularization terms
    user_bias_reg_term = 0.5 * Gamma * np.sum(bias_u**2)
    movie_bias_reg_term = 0.5 * Gamma * np.sum(bias_m**2)

    user_matrix_reg_term = 0
    for user_id in range(len(user_matrix)):
        user_matrix_reg_term += np.matmul(user_matrix[user_id], user_matrix[user_id])
    user_matrix_reg_term *= 0.5 * Tau

    movie_matrix_reg_term = 0
    for movie_id in range(len(movie_matrix)):
        movie_matrix_reg_term += np.matmul(movie_matrix[movie_id], movie_matrix[movie_id])
    movie_matrix_reg_term *= 0.5 * Tau

    loss = (- 0.5) * sum * Lambda - (user_bias_reg_term + movie_bias_reg_term + user_matrix_reg_term + movie_matrix_reg_term)

    return loss

#compute_loss_rmse(matrix_by_user,U, V, bias_u, bias_m)

"""calculate rmse"""

def compute_rmse(matrix_by_user, user_matrix, movie_matrix, bias_u, bias_m):
  loss = 0
  num_rating = 0
  for user_id in range(len(user_matrix)):
    for movie_id,rating in matrix_by_user[user_id]:
      loss += (rating - (user_matrix[user_id].T @ movie_matrix[movie_id] + bias_u[user_id] + bias_m[movie_id]))**2
      num_rating += 1

  mse = loss / num_rating
  rmse = np.sqrt(mse)
  return rmse

"""### Alterating Least square"""

def ALS_fit(U, matrix_by_user, matrix_by_movie, )



losses = []
rmses = []

for iteration in range(30):

  loss = compute_loss(matrix_by_user, matrix_by_movie, User_matrix, Movie_matrix, bias_u, bias_m)
  rmse = compute_rmse(matrix_by_user, User_matrix, Movie_matrix, bias_u, bias_m)
  losses.append(loss)

  #r = compute_rmse(U, V, matrix_by_user, bias_u, bias_m)
  rmses.append(rmse)

  User_matrix = update_user_matrices(matrix_by_user, User_matrix, Movie_matrix, bias_u, bias_m)
  Movie_matrix = update_movie_matrices(matrix_by_movie, User_matrix, Movie_matrix, bias_u, bias_m)
  bias_u = update_user_biases(matrix_by_user, User_matrix, Movie_matrix, bias_u, bias_m)
  bias_m = update_movie_biases(matrix_by_movie, User_matrix, Movie_matrix, bias_u, bias_m)

sns.lineplot(losses)
plt.xlim(-1, 30)
plt.xlabel('Number of iterations')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

sns.lineplot(rmses)
plt.xlim(-1, 30)
plt.xlabel('Number of iterations')
plt.ylabel('Rmse')
plt.grid(True)
plt.show()



"""## Buildind the fit function"""

import numpy as np

def fit_model(data_by_user_train, data_by_movie_train, data_by_user_test, data_by_movie_test,
              number_iterations=10, latent_factor_dimension=4):
    """
    Fit a collaborative filtering recommendation model.

    Args:
        train_data_by_user (list of lists): User-item interaction data for training.
        train_data_by_movie (list of lists): Item-user interaction data for training.
        test_data_by_user (list of lists): User-item interaction data for testing.
        test_data_by_movie (list of lists): Item-user interaction data for testing.
        number_iterations (int): Number of training iterations. Default is 10.
        folder_parameters (str): Folder path to load existing model parameters. Default is None.
        latent_factor_dimension (int): Dimension of the latent factors. Default is 4.

    This function trains a collaborative filtering recommendation model using the provided user-item interaction data.
    It uses the training data to learn the latent factors for users and items and to predict user-item interactions.
    The model can be trained from scratch or loaded from existing parameters if `folder_parameters` is specified.
    """
    sigma = np.sqrt(1/np.sqrt(latent_factor_dimension))  # Define sigma as a local variable

    number_of_users = len(data_by_user_train)
    number_of_movies = len(data_by_movie_train)


    # Initialize model parameters if not loading existing parameters
    User_matrix = np.random.normal(0, sigma, size=(number_of_users, latent_factor_dimension))
    Movie_matrix = np.random.normal  (0, sigma, size=(number_of_movies, latent_factor_dimension))
    bias_u = np.zeros(number_of_users)
    bias_m = np.zeros(number_of_movies)

    # Rest of the code remains the same

    loss_train, rmse_train, loss_test, rmse_test = [], [], [], []

    for i in range(number_iterations):

      # updating, bias and vectors
        User_matrix = update_user_matrices(data_by_user_train, User_matrix, Movie_matrix, bias_u, bias_m)
        Movie_matrix = update_movie_matrices(data_by_movie_train, User_matrix, Movie_matrix, bias_u, bias_m)
        bias_u = update_user_biases(data_by_user_train, User_matrix, Movie_matrix, bias_u, bias_m)
        bias_m = update_movie_biases(data_by_movie_train, User_matrix, Movie_matrix, bias_u, bias_m)


      # completing loss_train and rmse_train
        loss_train.append(compute_loss(data_by_user_train, data_by_movie_train, User_matrix, Movie_matrix, bias_u, bias_m))
        rmse_train.append(compute_rmse(data_by_user_train, User_matrix, Movie_matrix, bias_u, bias_m))

      # completing loss_test and rmse_test
        loss_test.append(compute_loss(data_by_user_test, data_by_movie_test, User_matrix, Movie_matrix, bias_u, bias_m))
        rmse_test.append(compute_rmse(data_by_user_test, User_matrix, Movie_matrix, bias_u, bias_m))


        print(f'iteration: {i}, loss_train: {loss_train[i]}, rmse_train: {rmse_train[i]}, '
              f'loss_test: {loss_test[i]}, rmse_test: {rmse_test[i]}')

    return loss_train, rmse_train, loss_test, rmse_test, User_matrix, Movie_matrix

loss_train, rmse_train, loss_test, rmse_test, User_matrix, Movie_matrix = fit_model(data_by_user_train, data_by_movie_train, data_by_user_test, data_by_movie_test,
              number_iterations=50)

"""## Plots

Loss
"""

def plot_loss(loss_train, loss_test, number_of_iterations):
    plt.plot(range(number_of_iterations), loss_train, color='orange')
    plt.plot(range(number_of_iterations), loss_test, color='blue')
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    plt.title('Loss_train vs loss_test')
    plt.grid(True, linestyle='--', alpha=0.3)
    plt.savefig('loss.pdf')
    plt.show()

plot_loss(loss_train, loss_test, 50)

"""RMSE"""

def plot_loss(rmse_train, rmse_test, number_of_iterations):
    plt.plot(range(number_of_iterations), rmse_train, color='orange')
    plt.plot(range(number_of_iterations), rmse_test, color='blue')
    plt.xlabel('Iteration')
    plt.ylabel('RMSE')
    plt.title('Root mean square error')
    plt.grid(True, linestyle='--', alpha=0.3)
    plt.savefig('rmse.pdf')
    plt.show()

plot_loss(rmse_train, rmse_test, 50)

"""## Dummy user"""

movie = pd.read_csv('/content/drive/MyDrive/ml-25m/ml-latest-small/movies.csv', index_col='movieId')

def recommend_dummy_user(movie_matrix, movie_system_id, movies, map_index_to_movie, top = 10):
    movie_id = map_index_to_movie.index(movie_system_id)
    map_index_to_movie = np.array(map_index_to_movie)
    similarity_array = []
    for i in range(len(movie_matrix)):
        similarity = np.dot(movie_matrix[movie_id], movie_matrix[i])
        similarity_array.append(similarity)
    indices = np.argpartition(similarity_array, -top)[-top:]
    movies_indices = map_index_to_movie[indices].astype(int)
    return movies.loc[movies_indices]

recommend_dummy_user(Movie_matrix, 1259, movie, list_movie_index)

"""## 2D plot"""

# Read movie data into a dictionary
def read_movie_data(file_path):
    dict_movies = {}
    with open(file_path, 'r') as data_file:
        data_reader = csv.reader(data_file)
        next(data_reader)  # Skip the header row
        dict_movies = {int(row[0]): row[1] for row in data_reader}
    return dict_movies

def search(movie_title, path_movie):
    dict_movies = read_movie_data(path_movie)

    # Find movies that match the keywords in the title
    keyword_matches = [(movie_id, title) for movie_id, title in dict_movies.items() if movie_title.lower() in title.lower()]

    if not keyword_matches:
        print(f"No movies found with keywords in the title: {movie_title}")
        return []
    return keyword_matches

def get_recommendations(movie_system_id, model):
    movie_matrix = model.movie_matrix
    movie_id = map_movie_to_index[movie_system_id]
    similarity_scores = np.dot(movie_matrix, movie_matrix[movie_id])
    top_indices = np.argsort(similarity_scores)[-10:]
    return top_indices

def plot_recommendations(movie_ids, model):
    plt.figure(figsize=(12, 8))
    dict_movies = read_movie_data(path_movie)

    # Define a list of colors and markers
    colors = ['b', 'g', 'r', 'y', 'c', 'm', 'k']
    markers = ['o', '^', 's', 'd', 'v', 'p', '*']

    # Determine the number of unique colors and markers needed based on the input length
    num_colors = min(len(movie_ids), len(colors))
    num_markers = min(len(movie_ids), len(markers))

    random.shuffle(colors)  # Shuffle the colors for variety
    random.shuffle(markers)  # Shuffle the markers for variety

    for i, movie_id in enumerate(movie_ids):
        top_recommendations = get_recommendations(movie_id, model)
        coordinates = model.movie_matrix[top_recommendations][:, [7, 3]]

        # Select a color and marker based on the current index
        color = colors[i % num_colors]
        marker = markers[i % num_markers]

        plt.scatter(coordinates[:, 0], coordinates[:, 1], marker=marker, c=color, label=f"Recommendations for Movie {dict_movies[movie_id]}")

        text_offset = 0.04
        for j, movie_idx in enumerate(top_recommendations):
            plt.annotate(dict_movies[map_index_to_movie[movie_idx]], (coordinates[j, 0] - text_offset, coordinates[j, 1] + text_offset), fontsize=8)

    plt.xlabel("X-coordinate")
    plt.ylabel("Y-coordinate")
    plt.title("Movie Recommendations")
    plt.legend()
    plt.grid(False)

    plt.subplots_adjust(left=0.08, right=0.95, top=0.92, bottom=0.08)
    plt.savefig("recommendation.pdf", format="pdf")
    plt.show()


# Define movie IDs and colors
movie_ids_and_colors = [1,1997,858,89745,209159]

# Call the plot_recommendations function
plot_recommendations(movie_ids_and_colors, model)  # Pass the model you have

"""## 1M datas"""

matrix_by_user_1M, matrix_by_movie_1M, list_user_index_1M, list_movie_index_1M = get_sparse_matrix(datas1M)

"""power law"""

power_law_plot(matrix_by_user_1M, matrix_by_movie_1M)

"""split"""

user_train_1M, user_test_1M, movie_train_1M, movie_test_1M = split_sparse_matrices(matrix_by_user_1M, matrix_by_movie_1M, 0.80)

user_sys_to_id, user_id_to_sys, movie_sys_to_id, movie_id_to_sys, data_by_user_train, data_by_movie_train, data_by_user_test, data_by_movie_test = create_sparse_matrix(datas1M)

k=[data_by_user_train, data_by_movie_train, data_by_user_test, data_by_movie_test]

for i in k :
  print (len(i))

"""Train"""

Lambda = 0.01
Gamma = 0.001
Tau = 0.01

loss_train, rmse_train, loss_test, rmse_test, User_matrix, Movie_matrix = fit_model(data_by_user_train, data_by_movie_train, data_by_user_test, data_by_movie_test,
              number_iterations=30)

"""plot"""

def plot_loss(loss_train, loss_test, number_of_iterations):
    plt.plot(range(number_of_iterations), loss_train, color='orange')
    plt.plot(range(number_of_iterations), loss_test, color='blue')
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    plt.title('Loss_train vs loss_test')
    plt.grid(True, linestyle='--', alpha=0.3)
    plt.savefig('loss.pdf')
    plt.show()

plot_loss(loss_train, loss_test, 30)

def plot_loss(rmse_train, rmse_test, number_of_iterations):
    plt.plot(range(number_of_iterations), rmse_train, color='orange')
    plt.plot(range(number_of_iterations), rmse_test, color='blue')
    plt.xlabel('Iteration')
    plt.ylabel('RMSE')
    plt.title('Root mean square error')
    plt.grid(True, linestyle='--', alpha=0.3)
    plt.savefig('rmse.pdf')
    plt.show()

plot_loss(rmse_train, rmse_test, 30)



"""## Bias only"""

import numpy as np
from tqdm import tqdm

def update_bias(data, bias, bias_to_change, m, lamda, gamma):
    # Update user or movie bias
    bias_sum = 0
    for index, rating in data[m]:
        bias_sum += (rating - bias[index])

    result = (lamda * bias_sum) / (lamda * len(data[m]) + gamma)
    bias_to_change[m] = result



def rmse(matrix_by_user, bias_u, bias_m):
  loss = 0
  num_rating = 0
  for user_id in range(len(matrix_by_user)):
    for movie_id,rating in matrix_by_user[user_id]:
      loss += (rating - ( + bias_u[user_id] + bias_m[movie_id]))**2
      num_rating += 1

  mse = loss / num_rating
  rmse = np.sqrt(mse)
  return rmse



def loss_func(data_by_user, data_by_movie, user_bias, movie_bias, lamda, gamma):
    # Calculate loss function with negative log-likelihood

    loss = 0
    nbr_rating = 0
    for i in range(len(data_by_user)):
        for movie, rating in data_by_user[i]:
            loss += (rating - user_bias[i] - movie_bias[movie]) ** 2
            nbr_rating += 1

    # Calculate user bias regularizer
    user_bias_sum = np.dot(user_bias, user_bias)

    # Calculate movie bias regularizer
    movie_bias_sum = np.dot(movie_bias, movie_bias)

    # Calculate the loglikelihood
    loglikelihood = -1 * (lamda / 2) * loss - 1 * (gamma / 2) * user_bias_sum - 1 * (gamma / 2) * movie_bias_sum

    rmse = np.sqrt(loss / nbr_rating)

    return -loglikelihood, rmse

def collaborative_filtering(latent_dimension, lamda, gamma, nbr_iterations, data_by_user_train, data_by_movie_train, data_by_user_test, data_by_movie_test):
    print('Initializing ...')
    number_of_users = len(data_by_user_train)
    number_of_movies = len(data_by_movie_train)
    sigma = 1 / np.sqrt(latent_dimension)

    user_bias = np.zeros(number_of_users)
    movie_bias = np.zeros(number_of_movies)

    losses_rmse = []
    losses_test_rmse = []

    iteration = []

    for i in range(nbr_iterations):
        print('Starting iteration', i+1)
        for user in tqdm(range(number_of_users), desc="Update user bias"):
            update_bias(data_by_user_train, movie_bias, user_bias, user, lamda, gamma)

        for movie in tqdm(range(number_of_movies), desc="Update movie bias"):
            update_bias(data_by_movie_train, user_bias, movie_bias, movie, lamda, gamma)

        loss = rmse(data_by_user_train, user_bias, movie_bias)
        loss_test = rmse(data_by_user_test, user_bias, movie_bias)
        losses_rmse.append(loss)
        losses_test_rmse.append(loss_test)


        iteration.append(i)

        print(f"Iteration {i + 1}, loss = {loss}, loss_test = {loss_test}")

    return     losses_rmse, losses_test_rmse

losses_rmse, losses_test_rmse = collaborative_filtering(1, 0.01, 0.001, 10, data_by_user_train, data_by_movie_train, data_by_user_test, data_by_movie_test)

losses_rmse

losses_test_rmse

def plot_loss(rmse_train,rmse_test, number_of_iterations):
    plt.plot(range(number_of_iterations), rmse_train, color='orange')
    plt.plot(range(number_of_iterations), rmse_test, color='blue')
    plt.xlabel('Iteration')
    plt.ylabel('RMSE')
    plt.title('Root mean square error')
    plt.grid(True, linestyle='--', alpha=0.3)
    plt.legend(labels=['Train', 'Test'])

    plt.savefig('rmse_bias.pdf')
    plt.show()

plot_loss(losses_rmse, losses_test_rmse, 10)

